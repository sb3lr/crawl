# README — أداة استخراج النطاقات الفرعية والروابط (مفصّل)

> **ملخص قصير:** أداة بايثون بسيطة لقراءة قائمة نطاقات فرعية، زحف كل نطاق فرعي باستخدام `gospider`، واستخراج كل الروابط الموجودة وتجميعها في ملفات منظمة داخل مجلد `output/`.

---

## ⚠️ تحذير قانوني

شغّل الأداة فقط على نطاقات لديك تصريح لاختبارها (نطاقات ملكيتك، أو ضمن نطاق برنامج Bug Bounty/Scope معلن). الاستخدام غير المرخّص غير قانوني ويمكن أن يعرّضك لمساءلة جنائية.

---

## الهدف والنتيجة

* **الهدف:** تحويل قائمة نطاقات فرعية إلى قائمة روابط كاملة قابلة للاستخدام في مراحل Recon والـ Fuzzing والاختبار اليدوي.
* **المخرجات الأساسية:**

  * `output/gospider_all.txt` — مخرجات gospider الخام (log + discovered links).
  * `output/all_urls.txt` — قائمة مرتبة وفريدة بكل الروابط المستخرجة من جميع النطاقات.

> ملاحظة: النسخة الحالية من السكربت تُنتج هذين الملفين كحد أدنى. يمكن تعديل السكربت بسهولة لإنتاج ملفات إضافية أو اختصارها حسب حاجتك.

---

## المتطلبات

1. Python 3
2. `gospider` متاح في PATH. إن لم يكن مثبتاً:

   ```bash
   go install github.com/jaeles-project/gospider@latest
   # أو: تثبيت من الحزمة المناسبة حسب النظام
   ```
3. ملف النطاقات الفرعية: `httpx.txt` (كل سطر: `sub.example.com`).
4. اتصال إنترنت ومستوى صلاحيات مناسب لتشغيل الأدوات.

---

## تركيب الملفات في المشروع

ضع الملفات التالية في مجلد المشروع:

```
crawl.py    # السكربت
httpx.txt              # قائمة النطاقات الفرعية (المدخل)
```

وعند التشغيل سيُنشئ المجلد `output/` إن لم يكن موجوداً ويضع الملفات داخله.

---

## كيفية التشغيل

1. تأكد من وجود `httpx.txt` في نفس مجلد السكربت.
2. شغّل:

```bash
python3 crawl.py
```

3. بعد اكتمال التنفيذ، افحص الملفات داخل `output/`.

---

## توضيح آلية عمل السكربت خطوة بخطوة (منطقياً)

1. **قراءة ملف النطاقات (`httpx.txt`)**: يُحمّل السكربت كل السطور الصالحة (يستبعد الفراغات).
2. **التحقق من وجود gospider**: يحاول تشغيل `gospider -h` للتأكد من أن الأداة متاحة.
3. **الزحف على كل نطاق فرعي**: لكل سطر في `httpx.txt` يشغّل `gospider` بالأعلام التالية:

   ```text
   gospider -s https://<sub> -c 2 -d 2 -t 10 -a -r
   ```

   ويجمع كل مخرجات الأمر (stdout و stderr) في `output/gospider_all.txt`.
4. **استخراج الروابط**: بعد اكتمال الزحف، يقرأ `gospider_all.txt` ويستخرج كل الURLs باستخدام تعبير منتظم، يُرتّبها ويُزيل الازدواج ثم يكتبها في `output/all_urls.txt`.

---

## تهيئة المتغيرات (في أعلى السكربت)

* `SUBS_FILE` — اسم ملف النطاقات (`httpx.txt`).
* `OUTPUT_DIR` — مجلد النتائج (`output`).
* `RAW_OUTPUT` — ملف مخرجات gospider.
* `ALL_URLS` — ملف الروابط النهائية.

يمكن تعديل هذه القيم مباشرة داخل السكربت لتغيير أسماء الملفات أو المسارات.

---

## أمثلة أوامر مساعدة بعد الإنتاج

* عدّ الملفات في الخرج:

```bash
ls output/*.txt | wc -l
```

* ترتيب وتفريغ الروابط الفريدة:

```bash
sort -u output/all_urls.txt -o output/all_urls.txt
```

* بحث سريع عن كلمات مفتاحية داخل الروابط:

```bash
grep -iE "admin|login|env|wp-" output/all_urls.txt
```

---

## حالات شائعة للمشاكل وكيفية إصلاحها

* **gospider غير موجود:** رسالة `gospider غير مثبت` — ثبّت الأداة أو أضفها إلى PATH.
* **ملف httpx.txt مفقود:** تأكد من وجود الملف واسمه صحيح.
* **مخرجات فارغة في all\_urls.txt:** تأكد من أن gospider نجح في الوصول للنطاقات (التحقق من `gospider_all.txt`) أو أن النطاقات حية.

---

## ملاحظات أمنية وممارسات أخلاقية

* لا تقم بمسح شامل دون إذن.
* احرص على عدم تعطيل الخدمات أثناء الزحف (تخفيض عدد الخيوط `-t` وزيادة التأخير).

---

